{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Create Documents </h1>\n",
    "\n",
    "This notebook will take the [Knowledge Graph](https://github.com/alexyoung13/frances_dissertation_ay55/blob/main/Notebooks/DataFrame2RDF_7thEdition.ipynb) and use some queries to create a documents, uris, and details files for all terms, and also just for topics and this is necessary for summariziation later. \n",
    "\n",
    "Currently this file is set up to read in the ttl file directly but can also be easily adapted to read from a Fueski server as evidenced by the last cell.\n",
    "\n",
    "Output:\n",
    "- terms_definitions.txt/topics_defintions.txt -> the definitions of either all terms or just topics\n",
    "- terms_details.txt/topics_details.txt: the article, edition number, the year, the volume number, the part number (optional), and letter of all terms or just topics \n",
    "- terms_uris.txt/topics_uris.txt -> the terms and topics uris fromt he KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from rdflib import Graph, URIRef, Literal, Namespace, XSD\n",
    "from rdflib.namespace import RDF, RDFS\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "g.parse('../data/edition7_clean.ttl', format='turtle')\n",
    "eb = Namespace(\"https://w3id.org/eb#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23122\n"
     ]
    }
   ],
   "source": [
    "#queries the KG for all terms\n",
    "\n",
    "query_string=\"\"\"\n",
    "PREFIX eb: <https://w3id.org/eb#>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "SELECT ?definition ?uri ?term ?vnum ?year ?enum ?letters ?part\n",
    "        WHERE {{\n",
    "    \t?uri a eb:Article .\n",
    "    \t?uri eb:name ?term .\n",
    "        ?uri eb:definition ?definition . \n",
    "        ?v eb:hasPart ?uri.\n",
    "        ?v eb:number ?vnum.\n",
    "        ?v eb:letters ?letters .\n",
    "        ?e eb:hasPart ?v.\n",
    "        ?e eb:publicationYear ?year.\n",
    "        ?e eb:number ?enum.\n",
    "        OPTIONAL {?v eb:part ?part; }\n",
    "        }\n",
    "  \t\tUNION {\n",
    "    \t?uri a eb:Topic .\n",
    "    \t?uri eb:name ?term . \n",
    "        ?uri eb:definition ?definition .\n",
    "        ?v eb:hasPart ?uri.\n",
    "        ?v eb:number ?vnum.\n",
    "        ?v eb:letters ?letters .\n",
    "        ?e eb:hasPart ?v.\n",
    "        ?e eb:publicationYear ?year.\n",
    "        ?e eb:number ?enum.\n",
    "        OPTIONAL {?v eb:part ?part; }\n",
    "        \n",
    "        }\n",
    "\n",
    "   } \n",
    "\"\"\" \n",
    "query = prepareQuery(query_string, initNs = { \"eb\": eb})\n",
    "# Execute the query on the graph\n",
    "results = g.query(query)\n",
    "print(len(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parses the query into the respective files\n",
    "\n",
    "documents=[]\n",
    "terms_info=[]\n",
    "uris=[]\n",
    "results_len = len(results)\n",
    "for r in results:\n",
    "    documents.append(r.definition.value)\n",
    "    uris.append(r.uri)\n",
    "    if r.part.value != \"Not specified\":\n",
    "        terms_info.append([r.term.value, r.enum.value, r.year.value, r.part.value, r.vnum.value, r.letters.value])\n",
    "    else:\n",
    "        terms_info.append([r.term.value, r.enum.value, r.year.value, \"\" , r.vnum.value, r.letters.value])\n",
    "\n",
    "with open('../data/terms_definitions.txt', 'wb') as fp:\n",
    "    pickle.dump(documents, fp)\n",
    "    \n",
    "with open('../data/terms_details.txt', 'wb') as fp2:\n",
    "    pickle.dump(terms_info, fp2)\n",
    "    \n",
    "with open('../data/terms_uris.txt', 'wb') as fp3:\n",
    "    pickle.dump(uris, fp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024\n"
     ]
    }
   ],
   "source": [
    "#queries for only topics for summarization\n",
    "\n",
    "query_string2=\"\"\"\n",
    "PREFIX eb: <https://w3id.org/eb#>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "SELECT ?definition ?uri ?term ?vnum ?year ?enum ?letters ?part\n",
    "        WHERE {\n",
    "    \t?uri a eb:Topic .\n",
    "    \t?uri eb:name ?term . \n",
    "        ?uri eb:definition ?definition .\n",
    "        ?v eb:hasPart ?uri.\n",
    "        ?v eb:number ?vnum.\n",
    "        ?v eb:letters ?letters .\n",
    "        ?e eb:hasPart ?v.\n",
    "        ?e eb:publicationYear ?year.\n",
    "        ?e eb:number ?enum.\n",
    "        OPTIONAL {?v eb:part ?part; }\n",
    "        \n",
    "        }\n",
    "\"\"\" \n",
    "query2 = prepareQuery(query_string2, initNs = { \"eb\": eb})\n",
    "# Execute the query on the graph\n",
    "results2 = g.query(query2)\n",
    "print(len(results2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts queries into their respective files\n",
    "\n",
    "documents=[]\n",
    "topics_info=[]\n",
    "uris=[]\n",
    "for r in results2:\n",
    "    documents.append(r.definition.value)\n",
    "    uris.append(r.uri)\n",
    "    if r.part.value != \"Not specified\":\n",
    "        topics_info.append([r.term.value, r.enum.value, r.year.value, r.part.value, r.vnum.value, r.letters.value])\n",
    "    else:\n",
    "        topics_info.append([r.term.value, r.enum.value, r.year.value, \"\" , r.vnum.value, r.letters.value])\n",
    "\n",
    "with open('../data/topics_definitions.txt', 'wb') as fp:\n",
    "    pickle.dump(documents, fp)\n",
    "    \n",
    "with open('../data/topics_details.txt', 'wb') as fp2:\n",
    "    pickle.dump(topics_info, fp2)\n",
    "    \n",
    "with open('../data/topics_uris.txt', 'wb') as fp3:\n",
    "    pickle.dump(uris, fp3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is example code of how to write the above code as way to parse from a fuseki server instead of directly loading the ttl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "##For the query cells replace the query with the lines:\n",
    "#\n",
    "# sparql.setQuery(query)\n",
    "# sparql.setReturnFormat(JSON)\n",
    "# results = sparql.query().convert()\n",
    "\n",
    "\n",
    "#for the parsing cells replace the parsing with the lines:\n",
    "#\n",
    "# documents=[]\n",
    "# terms_info=[]\n",
    "# uris=[]\n",
    "# results_len = len(results)\n",
    "# for r in results[\"results\"][\"bindings\"]:\n",
    "#     documents.append(r[\"definition\"][\"value\"])\n",
    "#     # uris.append(r[\"uri\"][\"value\"])\n",
    "#     # if \"part\" in r:\n",
    "#     #     terms_info.append([r[\"term\"][\"value\"], r[\"enum\"][\"value\"], r[\"year\"][\"value\"], r[\"part\"][\"value\"], r[\"vnum\"][\"value\"], r[\"letters\"][\"value\"]])\n",
    "#     # else:\n",
    "#     #     terms_info.append([r[\"term\"][\"value\"], r[\"enum\"][\"value\"], r[\"year\"][\"value\"], \"\" , r[\"vnum\"][\"value\"], r[\"letters\"][\"value\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
